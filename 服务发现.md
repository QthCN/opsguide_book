# 服务发现

## 什么是服务发现

服务发现（Service Discovery）并不是一个新的名词，在很多领域都有这个概念。例如读者想要使用公司的打印机，那么就得去『发现』打印机才行。而这个过程一般是新的打印机接入网络，将自身信息注册到某个资源库中，当你需要打印的时候就去查询这个资源库，然后就知道具体的打印机信息，接着就去连接打印机并走相关的打印协议进行打印。虽然目前我们的OGP平台已经可以部署容器了，但在现实的生产环境中很少有『单打独斗』的容器。一般规模较大的企业中其业务系统都会进行拆分，例如对于一个电商网站，他们的系统可能就会拆分为会员、交易、支付等等子系统，当用户下了一个订单后，这些子系统必须相互配合、相互调用才能处理这个订单。为什么要将一个系统进行拆分呢？原因主要是如果这些模块都在一起的话，对于代码的维护、性能的水平扩展等等都会造成问题。这些子系统之间一般通过RPC机制进行相互的调用，例如可以通过Dubbo、HFS或者走MQ的方式进行调用。那么如果我们的OGP中也要支持这种应用应该如何支持呢？很自然的想法就是将要被调用的服务固定在某个主机上，然后另一个容器要调用这个服务时就在配置文件里写死这个服务的IP:Port地址。但这明显存在缺陷，如果提供服务的主机宕机了怎么办？此时我们得将这个服务容器在另一台主机中启动，然后修改所有使用了这个服务的容器的配置文件并重启这些容器，这个工作量在容器数目达到一个程度的时候是巨大的。那能否通过vip这类方法通过HA的方式提供服务呢（vip及HA在我们后面的高可用章节会讲到）？在传统的部署模式下这是可以的，但在容器集群中这种方式就不行了，原因在于当我们上了调度功能（调度功能在我们后面的调度章节中会讲到）后，容器可能运行在集群中的任何一台物理主机上，并且可能今天它运行在主机A上，明天运行在主机B上，因此我们无法通过传统的HA软件（例如Keepalived）进行配置。此时为了解决这个问题就需要服务发现了。

服务发现一般包括了下面步骤：

* 应用启动后，将自身的服务地址注册到某个资源库中。这种注册方式可以是应用自身注册，也可以是某个代理程序帮忙注册
* 当某个应用要用到这个服务时，请求资源库获取服务地址
* 应用通过获取到的服务地址调用服务

但这只是最简单的一个场景，并且不具备生产环境的可行性，因为它没有考虑到服务地址信息的变更，变更后的事件通知等等。因此我们按照如下的方式按照角色来说明服务发现所包含的内容。

资源库：

* 提供服务地址信息的存储
* 过期一定时间未心跳同步的服务地址
* 对服务提供方做健康检查
* 对于服务地址信息的变更向感兴趣的订阅方发送通知
* 对于多个容器提供同一个服务的情况，向外提供地址时保证负载均衡

服务提供方：

* 当具备对外提供服务能力时，向资源库注册服务
* 定期向资源库发送心跳包，确保资源库不将自己的服务信息标记为无效
* 当自身所处的主机变更时，同步资源库
* 当自身无法提供服务时，从资源库上注销自己的信息

服务调用方：

* 有能力从资源库获取到服务地址
* 当资源库发送服务地址变更通知时，处理该通知
* 周期性的从资源库同步服务地址信息

一般的，服务提供方就是一个应用，如果每个应有都要具备向资源库注册信息的能力的话对应用的开发者其实是个额外的负担，因此一般会有一个代理程序负责帮助服务提供方和资源库同步服务地址信息。因此我们还有一个服务代理的角色：

服务代理：

* 帮助服务提供方同步地址向资源库同步地址信息
* 定期对服务提供方做健康检查，保证其能正常提供服务

对于一个服务发现系统来说，其每个角色不一定都要提供上面列出的全部功能，因为这些功能中有些的目的是重复的。另外有时候『服务代理』这个角色也不会存在。有些服务发现系统中的某些角色有一些其它的功能，或者会有一些其它角色存在，但大体上上面列出的就是大部分服务发现系统所提供的功能及职责清单了。

## 参考

本节我们来看下目前一些提供服务发现系统的平台中是如何提供该功能的。下面的例子很多来自于对应项目的官方文档。

### Consul

Consul由HashiCorp公司开发，它提供了很多的功能。通过它用户可以做到：

* 服务发现
* 服务的健康检查
* KV存储

其在KV存储方面提供了类似ZooKeeper、Etcd的功能，在健康检查方面提供了类似Nagio的功能，在服务发现方面提供了类似SkyDNS、SmartStack的功能。因此对于一些想减少环境中依赖的第三方组件的公司来说，Consul是一个值得认真考虑的选择。这里我们来看下其服务发现和监控检查方面的实现方式。首先我们先来了解一下Consul的组件。Consul中最重要的组件称为『Agent』，Agent有两种类型，一种是Server类型，用于提供真正的服务，包括提供KV存储及处理查询等，还有一种是Client类型。对于Server类型的Agent，为了高可用考虑一般都会运行多个，这些Server类型的Agent通过Raft算法（Raft算法我们在后面的章节会讲到）进行分布式数据同步。对于Client类型的Agent则主要负责响应客户发过来的请求，并把这些请求发给Server类型的Agent，由后者进行实际的处理。在集群中所有的主机都必须运行一个agent，一般会运行3、5、7个Server类型的Agent，其余的主机则运行Client类型的Agent。

现在来看下如何向Consul注册我们的服务地址信息。注册的方式很简单，当某台主机A上运行了某个服务时，可以在这个主机A的一个目录下写上这个服务的信息，然后向Client类型的Agent发送一个SIGHUP信号即可。SIGHUP可以通过kill命令发送，其在linux的信号语义中指的是让某个进程重新加载配置：

```bash
$ sudo mkdir /etc/consul.d
$ echo '{"service": {"name": "web", "tags": ["rails"], "port": 80}}' \
    >/etc/consul.d/web.json
$ consul agent -dev -config-dir /etc/consul.d
==> Starting Consul agent...
...
    [INFO] agent: Synced service 'web'
...
```

这里我们的主机A上运行了rails服务，rails大家可以认为是Python中的Django。其监听在主机的80端口。此时服务消费者可以通过A:80这个地址访问rails提供的服务。当我们把这个信息放到/etc/consul.d的目录后我们启动我们的Client类型的agent，并通过-config-dir指定这个目录，此时Client类型的Agent就会知道这个信息，并将这个信息同步给Server类型的Agent。那么服务消费方如何从Server获取到这个rails服务的地址信息呢？有两种方式可以获取，一种是通过DNS方式，还有一种是通过HTTP API的方式。

对于DNS方式，每个Client类型的Agent本身在启动后会提供DNS服务，这个时候我们可以通过域名的方式调用我们的rails服务。这个流程是这样的：

* 服务消费方向本机的Client发送DNS解析请求，请求的域名信息为NAME.service.consul，这里的NAME就是我们web.json中的name，也就是web。因此我们向Client请求解析web.service.consul对应的地址信息
* 本机的Client查看本地是否有这个DNS的信息，如果没有则去询问Server，接着将解析到的地址信息交给服务消费方

这里读者可能有个疑问：DNS是域名和IP的对应关系，那么通过web.service.consul获取到主机A的IP地址信息很容易理解，但这里的端口80怎么获取到呢？其实DNS中的记录有好几种类型，例如A类型就是常见的IP信息，而SRV类型则可以包括端口信息：

```bash
$ dig @127.0.0.1 -p 8600 web.service.consul SRV
...

;; QUESTION SECTION:
;web.service.consul.        IN  SRV

;; ANSWER SECTION:
web.service.consul. 0   IN  SRV 1 1 80 Armons-MacBook-Air.node.dc1.consul.

;; ADDITIONAL SECTION:
Armons-MacBook-Air.node.dc1.consul. 0 IN A  172.20.20.11
```

此时如果主机A宕机，那么我们的Server(即我们的资源库)需要通过监控检查的方式知道这个服务已经不在主机A上了。这种监控检查也是通过配置的方式进行配置的：

```bash
vagrant@n2:~$ echo '{"service": {"name": "web", "tags": ["rails"], "port": 80,
  "check": {"script": "curl localhost >/dev/null 2>&1", "interval": "10s"}}}' \
  >/etc/consul.d/web.json
```

这里我们在web.json中添加了check节，通过interval和script我们指定了每个10秒Client需要通过curl命令访问下本机的80端口，如果访问正常那么说明服务是OK的，否则Client就会发现这个服务无法正常提供功能，此时其会通过Server，而Server则会在其KV库中删除web这个rails服务的地址信息，也就是注销这个服务的地址信息。

如果这个服务要做迁移，从主机A迁移到了主机B，那么方法也类似：在主机A上移除web.json文件并向Client发送SIGHUP命令，在主机B上创建web.json文件并向Client发送SIGHUP命令，此时主机A和主机B上的Client就会正确的和Serer做数据同步，这是再向集群中的每个主机的Client发送DNS解析请求就会得到主机B的地址信息了。

需要注意的是如果通过DNS获取服务地址信息的话，需要考虑DNS缓存造成的影响。

另外对于HTTP方式也很简单，本机的Client会像提供DNS服务一样提供HTTP服务，此时调用对应的HTTP接口即可获取到服务地址信息：

```bash
$ curl http://localhost:8500/v1/catalog/service/web
[{"Node":"Armons-MacBook-Air","Address":"172.20.20.11","ServiceID":"web", \
    "ServiceName":"web","ServiceTags":["rails"],"ServicePort":80}]
```

可以看到Consul的Server、Clien具有非常好的扩展性。健康检查这个很耗资源的操作在Consul中是由每台主机的Client进行的，这样大大减少了Server的压力。但如果读者将上面给出的例子的流程和我们上面说的服务发现中各角色的职责相比较的话读者会发现这里有一个职责Consul没有提供：主动注册。当一个服务上线后我们需要手动的通过编写web.json及发送SIGHUP的方式告知Server我们的服务信息变更了。为了解决这个问题就需要用另外一个工具：Registrator。

### Registrator

Registrator是面向Docker容器的。在我们上面介绍的Consul中其有一个功能没有提供，那就是Registrator。虽然我们可以通过编写应用的方式自动的感知服务的变更并自动维护web.json文件，但如果有现成的工具的话很多公司还是会考虑现成工具的。Registrator的原理很简单：其会监听docker daemon上容器的启停操作。如果某个容器启动了，那么Registrator就会向我们的服务发现资源库中注册这个服务的信息，如果某个容器停止了那么其就会从资源库中移除这个服务的信息。对于资源库，Registrator支持Consul、Etcd、SkyDNS2、ZooKeeper等，并提供了接口可以供其它的资源库接入。Registrator对于服务信息的感知主要是通过docker启动时expose的端口及-p命令映射的端口感知的，而服务名则从容器名中解析。下面是个简单的例子，其中资源库使用的是Consul：

```bash
$ docker run -d -P --name=redis redis
$ curl $(boot2docker ip):8500/v1/catalog/services
{"consul":[],"redis":[]}
$ curl $(boot2docker ip):8500/v1/catalog/service/redis
[{"Node":"boot2docker","Address":"10.0.2.15","ServiceID":"boot2docker:redis:6379","ServiceName":"redis","ServiceTags":null,"ServiceAddress":"","ServicePort":32768}]
$ docker rm -f redis
redis
$ curl $(boot2docker ip):8500/v1/catalog/service/redis
[]
```

这里我们首先启动了一个redis的容器，接着按照我们上节将的访问HTTP的方式访问Consul Client的HTTP接口我们可以获取到redis服务的信息。接着我们停止并删除redis容器，此时再次访问Consul会发现这个服务的信息已经注销了。


### Etcd

Etcd主要提供KV存储、分布式锁等功能。在Etcd之前，最出名的分布式锁服务要数google的Chubby和apache的ZooKeeper了。这里我们主要看下如何使用Etcd来实现服务发现。

和Consul一样，由于Etcd可以提供KV存储的功能，因此当某个主机上启动了某个服务后，这个服务可以将自身的信息注册到Etcd上去，这样其它的服务消费者就可以从Etcd中查询某个服务的服务地址信息。不过这里的注册操作Etcd并不提供，需要服务提供方自己实现或者是借助Registrator来实现。和Consul不同的地方在于Etcd对于服务健康的检查方式以及服务提供方服务地址信息改变时服务消费方感知到这一信息的方式。我们来看个例子(这个例子来自于https://blog.gopheracademy.com/advent-2013/day-06-service-discovery-with-etcd/)。

首先我们的前端服务（fe1和fe2）分别运行在主机10.0.1.1和10.0.1.2上，为了让服务消费者可以发现这两个服务地址，我们需要把这些信息存放到我们的资源库Etcd中：

```bash
$ curl -L http://127.0.0.1:4001/v2/keys/frontends/fe1 -XPUT -d value=10.0.1.1
 {"action":"set","node":{"key":"/frontends/fe1","value":"10.0.1.1","modifiedIndex":2,"createdIndex":2}}
$ curl -L http://127.0.0.1:4001/v2/keys/frontends/fe2 -XPUT -d value=10.0.1.2
 {"action":"set","node":{"key":"/frontends/fe2","value":"10.0.1.2","modifiedIndex":4,"createdIndex":4}}
```

可以看到在Etcd中与Etcd的交互可以通过HTTP协议以及JSON来完成。现在上面的命令以及注册了两个前端服务，服务消费者只要通过如下的请求即可获取前端服务的地址信息：

```bash
$ curl -L http://127.0.0.1:4001/v2/keys/frontends/
 {"action":"get","node":{"key":"/frontends","dir":true,"nodes":[{"key":"/frontends/fe1","value":"10.0.1.1","modifiedIndex":3,"createdIndex":3},{"key":"/frontends/fe2","value":"10.0.1.2","modifiedIndex":4,"createdIndex":4}],"modifiedIndex":2,"createdIndex":2}}
```

那么如果我们某个前端服务的服务地址变更了，消费者和资源库如何能感知到这些信息呢？如果是基于Etcd来做服务发现的话，需要服务提供方或者服务代理定期的在Etcd中更新服务地址信息，并且Etcd中的信息需要有个过期时间。打个比方这里的fe2的服务地址信息会每隔3秒由服务提供方向Etcd同步，并且在注册这个服务的时候我们指定超时时间为5秒，那么如果我们的服务超过5秒没有同步自身信息到Etcd，Etcd就会删除这个服务的注册信息。在Etcd中这个过期时间称为TTL（time to live）：

```bash
$ curl -L http://127.0.0.1:4001/v2/keys/frontends/fe2 -XPUT -d value=10.0.1.2 -d ttl=5
 {"action":"set","node":{"key":"/frontends/fe2","prevValue":"10.0.1.2","value":"10.0.1.2","expiration":"2013-12-04T16:56:54.123531985-05:00","ttl":5,"modifiedIndex":5,"createdIndex":5}}
```

这里我们注册fe2的时候给了一个ttl的参数，让我们等待5秒，然后再执行查询接口：

```bash
$ curl -L http://127.0.0.1:4001/v2/keys/frontends/
 {"action":"get","node":{"key":"/frontends","dir":true,"nodes":[{"key":"/frontends/fe1","value":"10.0.1.1","modifiedIndex":3,"createdIndex":3}],"modifiedIndex":2,"createdIndex":2}}
```

可以看到fe2的服务地址信息已经看不到了。

相比较于使用DNS获取地址的方案，通过HTTP请求获取服务地址信息虽然是个办法，但有一个问题要注意：当资源库上的服务地址信息改变后，服务消费者如何感知这种改变并使用新的地址信息？一种方式是服务消费方每次调用服务的时候都去请求Etcd获取服务地址，但这个效率过于低下，并且Etcd不像Consul一样有Client Agent分担Etcd的负担。但Etcd提供了watch的功能，当服务地址变更的时候，如果服务消费者watch了这个服务目录，那么其就会收到通知。我们来看个例子：

```go
package main

import (
    "github.com/coreos/go-etcd/etcd"
    "log"
)

func main() {
    client := etcd.NewClient([]string{"http://127.0.0.1:4001"})
    resp, err := client.Get("frontends", false, false)
    if err != nil {
        log.Fatal(err)
    }
    log.Printf("Current frontends: %s: %s\n", resp.Node.Key, resp.Node.Value)
    watchChan := make(chan *etcd.Response)
    go client.Watch("/frontends", 0, false, watchChan, nil)
    log.Println("Waiting for an update...")
    r := <-watchChan
    log.Printf("Got updated frontends: %s: %s\n", r.Node.Key, r.Node.Value)
}
```

这里我们启动一个goroutine并在启动运行client.Watch监听/frontends目录的变化，如果这个目录变化了，例如fe1的地址信息改变了，那么watchChan这个channel就会被传入新的服务地址信息。通过这种方法服务消费者就不用不停的去询问Etcd了。

但有些读者可能会有疑问，如果按照这种方法的话，服务消费方很可能会有几秒的时间获取不到正确的服务地址信息，因为服务提供方是每隔一段时间才同步自身信息的。这在某些业务场景下可能会有问题。对于这类业务笔者认为使用ZooKeeper比较合适，因为ZooKeeper在Session中断的时候回触发事件，而Etcd虽然也能触发事件给watcher，但这个事件只是在TTL过期或目录信息变更的时候才触发。

### SkyDNS

SkyDNS的在服务发现方面的功能类似于Consul的通过DNS方式提供服务地址的功能。SkyDNS需要一个后端用于存放DNS的映射信息，可以选择Etcd作为其后端。其做服务发现的原理和我们上面举例的Consul比较类似，只不过对于Consul来说KV存储它自己做掉了，而SkyDNS则依赖一个外部的存储。当一个服务需要注册自己的服务地址到资源库的时候，可以调用如下的命令进行注册：

```bash
curl -XPUT http://127.0.0.1:4001/v2/keys/skydns/local/skydns/dns/ns/ns1 \
    -d value='{"host":"192.168.0.1"}'
```

注册完之后就可以以SkyDNS服务作为DNS解析服务，向其请求域名对应的实际地址信息。例如：

```bash
% dig @localhost SRV 1.rails.production.east.skydns.local

;; ANSWER SECTION:
1.rails.production.east.skydns.local. 3600 IN SRV 10 0 8080 service1.example.com.
```

这里读者可以发现基本上SkyDNS和Consul在DNS这块是一样的。区别在于Consul提供了更加多的功能。从基本功能来说SkyDNS只不过是一个类似于dnsmasq这类的提供DNS解析的工具，和传统的dnsmasq的区别在于其将域名的映射信息存储在了KV分布式存储上。因此如果读者在实际的生产或测试环境中要使用SkyDNS的话，需要依赖其它的工具（例如Registrator等）。或者Kubernetes支持在其服务发现中使用SkyDNS提供对外的DNS域名。但单独使用SkyDNS不是一个推荐的选择。

### SmartStack

SmartStack是Airbnb开发的一款服务发现工具，其主要由两个组件组成：Nerve和Synapse。这两个组件做的事情和我们上面的Registrator、Consul等其实差不多，但SmartStack有一个可以借鉴的地方是其没有通过DNS来映射地址，而是通过HAProxy通过虚拟IP的方式进行请求地址和实际地址的映射。由于使用了HAProxy因此其能做到非常好的负载均衡，并且HAProxy类似于Consul中的Client Agent，其在每台主机中都有部署，因此请求的压力也被分担了，且不会出现单点问题。但也正是引入了HAProxy，而HAProxy的配置文件需要手工编写，因此SmartStack需要借助Chef等配置管理工具的帮助来维护其配置文件（关于Chef等配置管理工具我们在后面的章节会进行介绍）。我们来详细的看下SmartStack。

首先来看下Nerve。当某台主机上运行一个服务www-bingotree的时候，我们可以在这台主机上启动这个服务，并在Nerve中配置下这个服务的信息，例如这个服务的名字叫www-bingotree,监听的主机IP是100.100.100.100，监听的主机端口是8080.同时我们可以在Nerve中配置健康检查的命令，这点和Consul是一样的。在Airbnb中服务的健康检查一般要求服务提供一个/health的接口，例如发送curl -X GET service_ip/health命令后如果返回的状态码是200那么说明服务一切正常，否则则判定为服务无法正常提供功能。通过这个方法Nerve可以准确的知道当前集群中有哪些服务，并且能准确的知道这些服务中哪些服务是工作正常的，在这之后Nerve会把这些信息存放到ZooKeeper中。

接着我们来看Synapse。Synapse做的事情就是配置HAProxy，其会用我们上面在Etcd中讲的watch的方法监听ZooKeeper中服务地址相关的目录，让Nerve更新了某个服务地址后，Synapse立刻知道某个服务的地址变动了，接着它就会对本地的HAProxy的配置文件进行修改，然后让HAProxy重新加载配置文件。

从这里我们可以看到在SmartStack中服务地址的源信息其实是存放在配置文件中并被Nerve读取的，这些源信息的维护在SmartStack中通过Chef进行维护，当然也可以通过SaltStack、Puppet等。

相比较于DNS的方式，通过HAProxy这种专门的组件提供映射确实有其优点，例如基于虚拟IP的方式就不用担心DNS cache的问题。但如果我们使用Consul那么我们只需要Consul即可，但如果使用SmartStack则我们还得单独在每台主机上维护一个HAProxy，这一点读者在实际环境中需要考虑到。

### Kubernetes

这里我们来看下服务发现在Kubernetes中的实现。在前面章节介绍Kubernetes的时候我们的测试环境搭建脚本中有用到一个service的概念，这个service在Kubernetes中就是指的一个服务。在Kubernetes中，服务的访问主要依赖一个叫做kube-proxy的组件。

kube-proxy在集群中每台主机中都存在。当我们通过kubectl创建一个服务后，Kubernetes使用的Etcd中就会有这个服务的实际服务提供方的信息，在Kubernetes中这个服务提供方地址称为Endpoint，这和OpenStack是一样的名字。当Etcd中服务目录添加了这个信息后，kube-proxy就会watch到这个事件，于是他们就会在其内存结构中记录这个映射关系。接着对于Kubernetes里Pod中的容器，当其要访问某个服务时会请求这个服务的虚拟IP，然后这个请求走宿主机的iptables转发到kube-proxy，而kube-proxy则会将这个请求的目的地址改为实际的服务提供方地址，这样请求就从容器流到了实际的服务提供方容器处了。可以看到这里kube-proxy其实起到了和在SmartStack中HAProxy一样的作用，并且实现的原理也类似，都是监听如Etcd、ZooKeeper之类的KV存储，然后修改本地映射关系，并处理请求的转发。在实际使用中供服务消费方使用的服务虚拟IP对于业务来说不是一个很容易记忆的东西，因此Kubernetes提供了DNS或公网IP的方式取代这个虚拟IP。

由于Kubernetes负责容器的创建及删除，并且其也会监听容器的健康状态，必要时对容器进行迁移，因此就省去了类似Registrator及配置健康检查脚本之类的工具及操作。

### 多播与名字指针

在分布式的书籍中，一般会提到多播与名字指针这两种服务发现（也叫『名字发现』）。其中：

* 多播：服务自动的向集群中的其它主机广播或多播自己的服务信息
* 名字指针：起始时服务运行在主机A上，服务调用方连接A即可获取到信息。后来服务从A迁移到了B，此时主机A上会有一个指针指向新的服务地址。当服务调用方访问A的时候，A告诉其目前服务可能在B，请去B处试下，则服务调用方就会访问B去获取服务。以此类推可能服务目前在C上运行，则B会告诉服务调用方去访问C

## 防火墙问题

服务发现在实际的生产环境中，会遇到一些安全方面的问题，其中最突出的就是『防火墙』策略问题。在一些企业（尤其是银行金融领域）防火墙策略是限制的非常严格的。如果前端服务器A只和后端服务器B通信，那么防火墙只会打开对应的A到B的某个服务端口。如果B上的服务挂了，我们的平台将上面的服务自动的迁移到了另一台主机C，此时即便A能发现此时服务的目的地址是C，但由于A与C之间网络不通，则A是无法获取到C上的服务的。

针对这类问题解决的方法一般有下面几种：

* 事先在一些服务器上都开通同样的防火墙策略，并且开通一段保留的端口。集群调度器在调度服务的时候，只使用这些服务器并只会使用保留的端口。这可能需要调度器支持『Label』的概念，这个在本书后面会详细说明。
* 防火墙开通逻辑纳入到调度器逻辑中，当服务运行到C上时，自动的调用防火墙的接口在C上开通防火墙策略，并在B上关闭防火墙策略。这可能需要防火墙设备的支持，或者需要基于SDN。关于SDN在本书后面会详细说明。

## 实现

现在我们开始实现OGP中的服务发现功能。
